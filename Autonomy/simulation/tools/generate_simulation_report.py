#!/usr/bin/env python3

"""
Simulation Adequacy Report Generator

This tool generates comprehensive reports on simulation adequacy for SLAM
and navigation testing, helping determine if Gazebo simulation provides
a sufficient analog for real hardware development.
"""

import json
import os
import time
import math
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from pathlib import Path
import numpy as np


@dataclass
class SLAMPerformanceMetrics:
    """SLAM performance metrics from test results."""
    pose_drift: float = 0.0  # meters over test distance
    loop_closure_success: float = 0.0  # percentage
    waypoint_accuracy: float = 0.0  # meters
    feature_count: int = 0
    map_consistency: float = 0.0  # meters
    pose_update_rate: float = 0.0  # Hz
    confidence: float = 0.0  # 0-1


@dataclass
class NavigationCapabilityMetrics:
    """Navigation capability metrics from test results."""
    waypoint_success_rate: float = 0.0  # percentage
    obstacle_avoidance_success: float = 0.0  # percentage
    path_planning_quality: float = 0.0  # 0-1
    real_time_performance: bool = True
    collision_count: int = 0
    average_clearance: float = 0.0  # meters


@dataclass
class SensorFidelityMetrics:
    """Sensor fidelity metrics from quality analysis."""
    imu_fidelity: float = 0.0
    gps_fidelity: float = 0.0
    lidar_fidelity: float = 0.0
    camera_fidelity: float = 0.0
    depth_camera_fidelity: float = 0.0
    overall_fidelity: float = 0.0


@dataclass
class SimulationLimitations:
    """Identified simulation limitations."""
    missing_physical_effects: List[str]
    sensor_model_simplifications: List[str]
    computational_differences: List[str]
    terrain_interaction_accuracy: float  # 0-1
    environmental_factors: List[str]


@dataclass
class SimulationAdequacyReport:
    """Complete simulation adequacy assessment report."""
    report_timestamp: float
    test_results_directory: str
    overall_adequacy_score: float  # 0-1
    adequacy_grade: str  # A, B, C, D, F
    recommendation: str  # Proceed, Validate Critical Paths, Hardware Required
    
    # Performance assessments
    slam_performance: SLAMPerformanceMetrics
    navigation_capability: NavigationCapabilityMetrics
    sensor_fidelity: SensorFidelityMetrics
    
    # Limitations and recommendations
    limitations: SimulationLimitations
    critical_tests_for_hardware: List[str]
    parameter_adjustments_needed: List[str]
    risk_areas: List[str]
    
    # Test coverage
    tests_completed: List[str]
    tests_failed: List[str]
    coverage_percentage: float


class SimulationReportGenerator:
    """Generator for comprehensive simulation adequacy reports."""

    def __init__(self, results_directory: str = "/tmp"):
        self.results_directory = Path(results_directory)
        self.report_timestamp = time.time()
        
    def generate_report(self) -> SimulationAdequacyReport:
        """Generate comprehensive simulation adequacy report."""
        print("Generating simulation adequacy report...")
        
        # Load test results
        test_results = self.load_test_results()
        
        # Load sensor quality data
        sensor_quality = self.load_sensor_quality_data()
        
        # Analyze SLAM performance
        slam_performance = self.analyze_slam_performance(test_results)
        
        # Analyze navigation capability
        navigation_capability = self.analyze_navigation_capability(test_results)
        
        # Analyze sensor fidelity
        sensor_fidelity = self.analyze_sensor_fidelity(sensor_quality)
        
        # Identify limitations
        limitations = self.identify_simulation_limitations(test_results, sensor_quality)
        
        # Calculate overall adequacy
        adequacy_score, grade, recommendation = self.calculate_adequacy(
            slam_performance, navigation_capability, sensor_fidelity
        )
        
        # Generate recommendations
        critical_tests, parameter_adjustments, risk_areas = self.generate_recommendations(
            slam_performance, navigation_capability, sensor_fidelity, limitations
        )
        
        # Create report
        report = SimulationAdequacyReport(
            report_timestamp=self.report_timestamp,
            test_results_directory=str(self.results_directory),
            overall_adequacy_score=adequacy_score,
            adequacy_grade=grade,
            recommendation=recommendation,
            slam_performance=slam_performance,
            navigation_capability=navigation_capability,
            sensor_fidelity=sensor_fidelity,
            limitations=limitations,
            critical_tests_for_hardware=critical_tests,
            parameter_adjustments_needed=parameter_adjustments,
            risk_areas=risk_areas,
            tests_completed=self.get_completed_tests(test_results),
            tests_failed=self.get_failed_tests(test_results),
            coverage_percentage=self.calculate_test_coverage(test_results)
        )
        
        # Save report
        self.save_report(report)
        
        # Print summary
        self.print_summary(report)
        
        return report
        
    def load_test_results(self) -> Dict[str, Any]:
        """Load test results from JSON files."""
        test_results = {}
        
        # Look for test result files
        for file_path in self.results_directory.glob("*_results_*.json"):
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                    test_name = data.get('test_name', 'unknown')
                    test_results[test_name] = data
            except Exception as e:
                print(f"Warning: Could not load {file_path}: {e}")
                
        return test_results
        
    def load_sensor_quality_data(self) -> Optional[Dict[str, Any]]:
        """Load sensor quality analysis data."""
        sensor_quality_file = self.results_directory / f"sensor_quality_report_{int(self.report_timestamp)}.json"
        
        if sensor_quality_file.exists():
            try:
                with open(sensor_quality_file, 'r') as f:
                    return json.load(f)
            except Exception as e:
                print(f"Warning: Could not load sensor quality data: {e}")
                
        return None
        
    def analyze_slam_performance(self, test_results: Dict[str, Any]) -> SLAMPerformanceMetrics:
        """Analyze SLAM performance from test results."""
        metrics = SLAMPerformanceMetrics()
        
        # Look for waypoint navigation test
        waypoint_test = test_results.get('autonomous_waypoint_navigation', {})
        if waypoint_test:
            slam_metrics = waypoint_test.get('slam_metrics', {})
            metrics.pose_drift = slam_metrics.get('pose_drift', 0.0)
            metrics.loop_closure_success = slam_metrics.get('loop_closure_success', 0.0)
            metrics.waypoint_accuracy = slam_metrics.get('waypoint_accuracy', 0.0)
            metrics.feature_count = slam_metrics.get('feature_count', 0)
            metrics.map_consistency = slam_metrics.get('map_consistency', 0.0)
            metrics.pose_update_rate = slam_metrics.get('pose_update_rate', 0.0)
            metrics.confidence = slam_metrics.get('confidence', 0.0)
            
        # Look for GPS-denied test
        gps_denied_test = test_results.get('gps_denied_slam', {})
        if gps_denied_test:
            drift_metrics = gps_denied_test.get('slam_drift_metrics', {})
            # Update metrics with GPS-denied specific data
            if drift_metrics.get('total_drift', 0) > 0:
                metrics.pose_drift = max(metrics.pose_drift, drift_metrics['total_drift'])
                
        return metrics
        
    def analyze_navigation_capability(self, test_results: Dict[str, Any]) -> NavigationCapabilityMetrics:
        """Analyze navigation capability from test results."""
        metrics = NavigationCapabilityMetrics()
        
        # Analyze waypoint navigation results
        waypoint_test = test_results.get('autonomous_waypoint_navigation', {})
        if waypoint_test:
            waypoints_completed = waypoint_test.get('waypoints_completed', 0)
            total_waypoints = waypoint_test.get('total_waypoints', 1)
            metrics.waypoint_success_rate = (waypoints_completed / total_waypoints) * 100.0
            
        # Analyze obstacle avoidance results
        obstacle_test = test_results.get('dynamic_obstacle_avoidance', {})
        if obstacle_test:
            metrics.obstacle_avoidance_success = obstacle_test.get('obstacle_avoidance_success', 0.0)
            metrics.collision_count = obstacle_test.get('collision_count', 0)
            metrics.average_clearance = obstacle_test.get('average_clearance', 0.0)
            
        # Analyze endurance test results
        endurance_test = test_results.get('endurance_slam_test', {})
        if endurance_test:
            metrics.real_time_performance = endurance_test.get('real_time_performance', True)
            
        return metrics
        
    def analyze_sensor_fidelity(self, sensor_quality: Optional[Dict[str, Any]]) -> SensorFidelityMetrics:
        """Analyze sensor fidelity from quality analysis."""
        metrics = SensorFidelityMetrics()
        
        if sensor_quality:
            sensor_reports = sensor_quality.get('sensor_reports', {})
            
            metrics.imu_fidelity = sensor_reports.get('imu', {}).get('fidelity_score', 0.0)
            metrics.gps_fidelity = sensor_reports.get('gps', {}).get('fidelity_score', 0.0)
            metrics.lidar_fidelity = sensor_reports.get('lidar', {}).get('fidelity_score', 0.0)
            metrics.camera_fidelity = sensor_reports.get('camera', {}).get('fidelity_score', 0.0)
            metrics.depth_camera_fidelity = sensor_reports.get('depth_camera', {}).get('fidelity_score', 0.0)
            metrics.overall_fidelity = sensor_quality.get('overall_fidelity_score', 0.0)
            
        return metrics
        
    def identify_simulation_limitations(self, test_results: Dict[str, Any], sensor_quality: Optional[Dict[str, Any]]) -> SimulationLimitations:
        """Identify simulation limitations and gaps."""
        limitations = SimulationLimitations(
            missing_physical_effects=[],
            sensor_model_simplifications=[],
            computational_differences=[],
            terrain_interaction_accuracy=0.8,  # Default assumption
            environmental_factors=[]
        )
        
        # Analyze sensor quality for limitations
        if sensor_quality:
            sensor_reports = sensor_quality.get('sensor_reports', {})
            
            for sensor_name, report in sensor_reports.items():
                fidelity_score = report.get('fidelity_score', 0.0)
                if fidelity_score < 0.7:
                    limitations.sensor_model_simplifications.append(
                        f"{sensor_name} simulation has significant simplifications (fidelity: {fidelity_score:.2f})"
                    )
                    
        # Analyze test results for limitations
        for test_name, results in test_results.items():
            if results.get('status') == 'failed':
                limitations.computational_differences.append(
                    f"{test_name} test failed, indicating computational differences"
                )
                
        # Add common simulation limitations
        limitations.missing_physical_effects.extend([
            "No dust or particle effects on sensors",
            "No vibration effects on IMU",
            "No temperature effects on sensor performance",
            "No electromagnetic interference modeling",
            "Simplified wheel-terrain interaction"
        ])
        
        limitations.environmental_factors.extend([
            "No dynamic lighting changes",
            "No weather effects",
            "No wind effects on navigation",
            "Static environment (no moving objects)"
        ])
        
        return limitations
        
    def calculate_adequacy(self, slam_performance: SLAMPerformanceMetrics, 
                          navigation_capability: NavigationCapabilityMetrics,
                          sensor_fidelity: SensorFidelityMetrics) -> tuple[float, str, str]:
        """Calculate overall simulation adequacy score and recommendation."""
        score = 0.0
        
        # SLAM performance weight: 40%
        slam_score = 0.0
        if slam_performance.pose_drift < 1.0:  # Less than 1m drift
            slam_score += 0.3
        if slam_performance.loop_closure_success > 95.0:  # High loop closure success
            slam_score += 0.3
        if slam_performance.confidence > 0.7:  # High confidence
            slam_score += 0.2
        if slam_performance.pose_update_rate > 5.0:  # Good update rate
            slam_score += 0.2
            
        score += 0.4 * slam_score
        
        # Navigation capability weight: 30%
        nav_score = 0.0
        if navigation_capability.waypoint_success_rate > 90.0:  # High success rate
            nav_score += 0.4
        if navigation_capability.obstacle_avoidance_success > 90.0:  # Good obstacle avoidance
            nav_score += 0.3
        if navigation_capability.real_time_performance:  # Real-time capable
            nav_score += 0.2
        if navigation_capability.collision_count == 0:  # No collisions
            nav_score += 0.1
            
        score += 0.3 * nav_score
        
        # Sensor fidelity weight: 30%
        score += 0.3 * sensor_fidelity.overall_fidelity
        
        # Convert to grade
        if score >= 0.9:
            grade = 'A'
            recommendation = "Proceed with Development - High confidence in simulation adequacy"
        elif score >= 0.8:
            grade = 'B'
            recommendation = "Proceed with Development - Good simulation adequacy with minor limitations"
        elif score >= 0.7:
            grade = 'C'
            recommendation = "Validate Critical Paths - Moderate simulation adequacy, test critical algorithms on hardware"
        elif score >= 0.6:
            grade = 'D'
            recommendation = "Validate Critical Paths - Limited simulation adequacy, extensive hardware validation needed"
        else:
            grade = 'F'
            recommendation = "Hardware Required - Simulation inadequate for development, use real hardware"
            
        return score, grade, recommendation
        
    def generate_recommendations(self, slam_performance: SLAMPerformanceMetrics,
                               navigation_capability: NavigationCapabilityMetrics,
                               sensor_fidelity: SensorFidelityMetrics,
                               limitations: SimulationLimitations) -> tuple[List[str], List[str], List[str]]:
        """Generate specific recommendations based on analysis."""
        critical_tests = []
        parameter_adjustments = []
        risk_areas = []
        
        # Critical tests based on performance gaps
        if slam_performance.pose_drift > 2.0:
            critical_tests.append("SLAM pose estimation accuracy validation on real hardware")
            
        if navigation_capability.waypoint_success_rate < 80.0:
            critical_tests.append("Waypoint navigation validation on real hardware")
            
        if sensor_fidelity.overall_fidelity < 0.7:
            critical_tests.append("Sensor data quality validation on real hardware")
            
        # Parameter adjustments
        if sensor_fidelity.imu_fidelity < 0.8:
            parameter_adjustments.append("Adjust IMU noise parameters to better match real sensors")
            
        if sensor_fidelity.gps_fidelity < 0.8:
            parameter_adjustments.append("Adjust GPS noise and update rate parameters")
            
        if slam_performance.pose_update_rate < 5.0:
            parameter_adjustments.append("Increase SLAM processing frequency")
            
        # Risk areas
        if slam_performance.confidence < 0.7:
            risk_areas.append("SLAM reliability in challenging environments")
            
        if navigation_capability.collision_count > 0:
            risk_areas.append("Obstacle avoidance performance")
            
        if sensor_fidelity.overall_fidelity < 0.6:
            risk_areas.append("Sensor data quality for SLAM algorithms")
            
        return critical_tests, parameter_adjustments, risk_areas
        
    def get_completed_tests(self, test_results: Dict[str, Any]) -> List[str]:
        """Get list of completed tests."""
        completed = []
        for test_name, results in test_results.items():
            if results.get('status') == 'completed':
                completed.append(test_name)
        return completed
        
    def get_failed_tests(self, test_results: Dict[str, Any]) -> List[str]:
        """Get list of failed tests."""
        failed = []
        for test_name, results in test_results.items():
            if results.get('status') == 'failed':
                failed.append(test_name)
        return failed
        
    def calculate_test_coverage(self, test_results: Dict[str, Any]) -> float:
        """Calculate test coverage percentage."""
        expected_tests = [
            'autonomous_waypoint_navigation',
            'gps_denied_slam',
            'dynamic_obstacle_avoidance',
            'endurance_slam_test',
            'sensor_failure_recovery'
        ]
        
        completed_tests = self.get_completed_tests(test_results)
        return (len(completed_tests) / len(expected_tests)) * 100.0
        
    def save_report(self, report: SimulationAdequacyReport):
        """Save report to file."""
        report_file = self.results_directory / f"simulation_adequacy_report_{int(self.report_timestamp)}.json"
        
        # Convert to serializable format
        report_dict = asdict(report)
        
        with open(report_file, 'w') as f:
            json.dump(report_dict, f, indent=2)
            
        print(f"Report saved to {report_file}")
        
    def print_summary(self, report: SimulationAdequacyReport):
        """Print report summary."""
        print("\n" + "="*80)
        print("SIMULATION ADEQUACY ASSESSMENT REPORT")
        print("="*80)
        print(f"Overall Adequacy Score: {report.overall_adequacy_score:.2f}")
        print(f"Grade: {report.adequacy_grade}")
        print(f"Recommendation: {report.recommendation}")
        print(f"Test Coverage: {report.coverage_percentage:.1f}%")
        
        print(f"\nSLAM Performance:")
        print(f"  Pose Drift: {report.slam_performance.pose_drift:.3f}m")
        print(f"  Loop Closure Success: {report.slam_performance.loop_closure_success:.1f}%")
        print(f"  Waypoint Accuracy: {report.slam_performance.waypoint_accuracy:.3f}m")
        print(f"  Confidence: {report.slam_performance.confidence:.2f}")
        
        print(f"\nNavigation Capability:")
        print(f"  Waypoint Success Rate: {report.navigation_capability.waypoint_success_rate:.1f}%")
        print(f"  Obstacle Avoidance Success: {report.navigation_capability.obstacle_avoidance_success:.1f}%")
        print(f"  Real-time Performance: {'Yes' if report.navigation_capability.real_time_performance else 'No'}")
        print(f"  Collision Count: {report.navigation_capability.collision_count}")
        
        print(f"\nSensor Fidelity:")
        print(f"  Overall Fidelity: {report.sensor_fidelity.overall_fidelity:.2f}")
        print(f"  IMU Fidelity: {report.sensor_fidelity.imu_fidelity:.2f}")
        print(f"  GPS Fidelity: {report.sensor_fidelity.gps_fidelity:.2f}")
        print(f"  LiDAR Fidelity: {report.sensor_fidelity.lidar_fidelity:.2f}")
        print(f"  Camera Fidelity: {report.sensor_fidelity.camera_fidelity:.2f}")
        
        print(f"\nKey Limitations:")
        for limitation in report.limitations.missing_physical_effects[:3]:
            print(f"  - {limitation}")
        if len(report.limitations.missing_physical_effects) > 3:
            print(f"  - ... and {len(report.limitations.missing_physical_effects) - 3} more")
            
        print(f"\nCritical Tests for Hardware Validation:")
        for test in report.critical_tests_for_hardware[:3]:
            print(f"  - {test}")
        if len(report.critical_tests_for_hardware) > 3:
            print(f"  - ... and {len(report.critical_tests_for_hardware) - 3} more")
            
        print("="*80)


def main():
    """Main function."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Generate simulation adequacy report')
    parser.add_argument('--results-dir', default='/tmp', 
                       help='Directory containing test results (default: /tmp)')
    
    args = parser.parse_args()
    
    generator = SimulationReportGenerator(args.results_dir)
    report = generator.generate_report()
    
    return report


if __name__ == '__main__':
    main()
